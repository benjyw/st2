name: ci

on:
  push:
    branches:
      # only on merges to master branch
      - master
      # and version branches, which only include minor versions (eg: v3.4)
      - v[0-9]+.[0-9]+
    tags:
      # also version tags, which include bugfix releases (eg: v3.4.0)
      - v[0-9]+.[0-9]+.[0-9]+
  pull_request:
    type: [opened, reopened, edited]
    branches:
      # Only for PRs targeting those branches
      - main
      - master
      - v[0-9]+.[0-9]+
  schedule:
    # run every night at midnight
    - cron:  '0 0 * * *'

jobs:
  ci:
    name: '${{ matrix.name }} - python (${{ matrix.python-version }})'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include:
          #- name: 'Lint Checks'
          #  task: 'ci-checks'
          #  python-version: '3.6'
          #- name: 'Compile'
          #  task: 'ci-compile'
          #  python-version: '3.6'
          #- name: 'Pack Tests'
          #  task: 'ci-packs-tests'
          #  python-version: '3.6'
          #- name: 'Unit Tests'
          #  task: 'ci-unit'
          #  python-version: '3.6'
          # - name: 'Micro Benchmarks'
          #   task: 'micro-benchmarks'
          #   python-version: '3.6'
          - name: 'Integration Tests'
            task: 'ci-integration'
            python-version: '3.6'
    services:
      mongo:
        image: mongo:4.0
        ports:
          - 27017:27017

      # In GHA, these services are started first before the code is checked out.
      # We use bitnami images to facilitate reconfiguring RabbitMQ during ci-integration tests.
      # We rely on custom config and SSL certs that are in the repo.
      # Many images require config in env vars (which we can't change during the test job)
      # or they require config in entrypoint args (which we can't override for GHA services)
      # bitnami builds ways to get config files from mounted volumes.
      rabbitmq:
        image: bitnami/rabbitmq:3.8
        volumes:
          - /home/runner/rabbitmq_conf:/bitnami/conf  # RABBITMQ_MOUNTED_CONF_DIR
        env:
          # tell bitnami/rabbitmq to enable this by default
          RABBITMQ_PLUGINS: rabbitmq_management
          # Since we're using a container, we can't use guest:guest
          # RABBITMQ_SECURE_PASSWORD: "no"
          # RABBITMQ_USERNAME: st2
          # RABBITMQ_PASSWORD: st2
          RABBITMQ_USERNAME: guest
          RABBITMQ_PASSWORD: guest

        # These are strictly docker options, not entrypoint args (GHA restriction)
        options: >-
          --name rabbitmq

        ports:
          # These 6 ports are exposed by bitnami/rabbitmq (see https://www.rabbitmq.com/networking.html#ports)
          # host_port:container_port/protocol
          - 5671:5671/tcp   # AMQP SSL port
          - 5672:5672/tcp   # AMQP standard port
          - 15672:15672/tcp # Management: HTTP, CLI
          #- 15671:15671/tcp # Management: SSL port
          #- 25672:25672/tcp # inter-node or CLI
          #- 4369:4369/tcp   # epmd

    env:
      TASK: '${{ matrix.task }}'

      # We need to explicitly specify terminal width otherwise some CLI tests fail on container
      # environments where small terminal size is used.
      COLUMNS: '120'
      PYLINT_CONCURRENCY: '2'

      # CI st2.conf
      #   - with ST2_CI_USER user instead of stanley
      #   - with RABBITMQ_USERNAME:RABBITMQ_PASSWORD instead of guest:guest
      ST2_CONF: 'conf/st2.ci.conf'

      # Tell StackStorm that we are indeed in CI mode, previously we hard coded a Travis specific
      # environment variable in our test code, making it a PITA when we switch CI providers.
      # Now, we simply set this environment varible here in the CI portion of our testing and
      # it avoids any CI provider type lock-in.
      ST2_CI: 'true'

      # Name of the user who is running the CI (on GitHub Actions this is 'runner')
      ST2_CI_USER: 'runner'

      # Since we're using a container, we can't use guest:guest
      # RABBITMQ_USERNAME: st2
      # RABBITMQ_PASSWORD: st2
    steps:
      - name: Custom Environment Setup
        # built-in GitHub Actions environment variables
        # https://docs.github.com/en/free-pro-team@latest/actions/reference/environment-variables
        #
        # setting environment variables, so we can use shell logic
        # https://docs.github.com/en/free-pro-team@latest/actions/reference/workflow-commands-for-github-actions#setting-an-environment-variable
        run: |
          IS_NIGHTLY_BUILD=$([ "${GITHUB_EVENT_NAME}" = "schedule" ] && echo "yes" || echo "no")
          echo "IS_NIGHTLY_BUILD=${IS_NIGHTLY_BUILD}" >> $GITHUB_ENV

          # NOTE: We only enable coverage for master builds and not pull requests
          # since it has huge performance overhead (tests are 50% or so slower)
          ENABLE_COVERAGE=$([ "${GITHUB_EVENT_NAME}" != "pull_request" ] && [ "${IS_NIGHTLY_BUILD}" = "no" ] && echo "yes" || echo "no")
          echo "ENABLE_COVERAGE=${ENABLE_COVERAGE}" >> $GITHUB_ENV

          # We only run tests with "--with-timer" flag on master and not for PRs since it adds 1-2
          # minutes of overhead to each build.
          NOSE_TIME=$([ "${GITHUB_EVENT_NAME}" != "pull_request" ] && [ "${IS_NIGHTLY_BUILD}" = "no" ] && echo "yes" || echo "no")
          echo "NOSE_TIME=${NOSE_TIME}" >> $GITHUB_ENV

          # Setup the path to the st2 repo in the CI build system
          echo "ST2_CI_REPO_PATH=${GITHUB_WORKSPACE}" >> $GITHUB_ENV
      - name: Checkout repository
        uses: actions/checkout@v2
      - name: 'Set up Python (${{ matrix.python-version }})'
        uses: actions/setup-python@v2
        with:
          python-version: '${{ matrix.python-version }}'
      - name: Get date components for use in cache-keys
        id: date
        run: |
          echo "::set-output name=year::$(/bin/date -u "+%Y")"
          echo "::set-output name=month::$(/bin/date -u "+%m")"
          echo "::set-output name=week::$(/bin/date -u "+%U")"
      - uses: actions/cache@v2
        with:
          path: |
            ~/.cache/pip
            virtualenv
          key: py-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('requirements.txt', 'test-requirements.txt') }}
          restore-keys: |
            py-${{ runner.os }}-${{ matrix.python }}-
      - uses: actions/cache@v2
        with:
          path: |
            /var/cache/apt/archives/*.deb
            /var/cache/apt/archives/partial/*.deb
            /var/cache/apt/*.bin
          key: apt-${{ runner.os }}-${{ steps.date.outputs.year }}-${{ steps.date.outputs.week }}
          restore-keys: |
            apt-${{ runner.os }}-${{ steps.date.outputs.year }}-
            apt-${{ runner.os }}-
      - name: Install apt depedencies
        run: |
          # install dev dependencies for Python YAML and LDAP packages
          # https://github.com/StackStorm/st2-auth-ldap
          sudo apt-get -y update
          sudo apt-get -f -y install libldap2-dev libsasl2-dev libssl-dev libyaml-dev ldap-utils
      - name: Install virtualenv
        run: |
          #ls /opt/hostedtoolcache/Python/3.6.13/x64/lib/python3.6/site-packages
          which virtualenv || true
          virtualenv --version || true
          # Note: Use the verison of virtualenv pinned in fixed-requirements.txt so we
          #       only have to update it one place when we change the version
          pip install --upgrade --force-reinstall $(grep "^virtualenv" fixed-requirements.txt)
          which virtualenv
          virtualenv --version
          #ls /opt/hostedtoolcache/Python/3.6.13/x64/lib/python3.6/site-packages
          #virtualenv --help # no config file
          ls -l /opt/hostedtoolcache/Python/3.6*/x64/bin/python*
          exit 1
      - name: Install requirements
        run: |
          ./scripts/travis/install-requirements.sh
      - name: Setup integration tests
        run: |
          # prep a ci-specific dev conf file that
          #   - uses runner instead of stanley
          #     this user is the username of the user in GitHub actions,
          #     used for SSH, etc during integration tests (important)
          #   - uses a non-guest user/pass for RabbitMQ since we can't use guest outside of the container
          cp conf/st2.dev.conf "${ST2_CONF}" ; sed -i -e "s/stanley/${ST2_CI_USER}/" "${ST2_CONF}"
          # cp conf/st2.dev.conf "${ST2_CONF}" ; sed -i -e "s/stanley/${ST2_CI_USER}/" -e "s/guest:guest/${RABBITMQ_USERNAME}:${RABBITMQ_PASSWORD}/" "${ST2_CONF}"
          scripts/travis/add-itest-user-key.sh
          sudo .circle/add-itest-user.sh
      - name: Permissions Workaround
        if: "${{ env.TASK == 'ci-packs-tests' || env.TASK == 'ci-integration' }}"
        run: |
          echo "$ST2_CI_REPO_PATH"
          sudo ST2_CI_REPO_PATH="${ST2_CI_REPO_PATH}" scripts/travis/permissions-workaround.sh
          # sudo chmod -R o+w /home/runner/.local/share/virtualenv
      - name: Finish Integration test setup (RabbitMQ, st2.tests.conf)
        if: "${{ env.TASK == 'ci-integration' }}"
        # bitnami image allows (see bitnami/rabbitmq readme):
        # Here we're copying a rabbitmq.config file which won't do anything.
        # We need to switch to custom.conf or advanced.config.
        timeout-minutes: 2  # may die if rabbitmq fails to start
        run: |
          set -x
          # Since we're using a container, we can't use guest:guest
          # Integration tests hardcode paths to conf/st2.tests*.conf so we modify in place.
          ###sed -i -e "s/guest:guest/${RABBITMQ_USERNAME}:${RABBITMQ_PASSWORD}/" conf/st2.tests*.conf
          # Use custom RabbitMQ config which enables SSL / TLS listener on port 5671 with test certs
          sudo cp scripts/github/rabbitmq.conf /home/runner/rabbitmq_conf/custom.conf
          # The code is checked out after the container is already up, so we don't mount them.
          # We copy those certs into the dir that is mounted to /bitnami/conf
          sudo cp -r st2tests/st2tests/fixtures/ssl_certs /home/runner/rabbitmq_conf/
          # refresh rabbitmq config - based on ENTRYPOINT logic
          docker exec rabbitmq bash -c 'cat /bitnami/conf/custom.conf >> /opt/bitnami/rabbitmq/etc/rabbitmq/rabbitmq.conf'
          # sleep to prevent interleaved output in GHA logs
          docker exec rabbitmq cat /opt/bitnami/rabbitmq/etc/rabbitmq/rabbitmq.conf && sleep 0.1
          echo
          echo restarting rabbitmq container
          docker restart rabbitmq
          # wait for rabbitmq container to restart
          until [ "$(docker inspect -f {{.State.Running}} rabbitmq)" == "true" ]; do sleep 0.1; done
          echo enabled RabbitMQ plugins:
          # print plugins list to: (1) ease debugging, (2) pause till rabbitmq is really running
          docker exec rabbitmq rabbitmq-plugins list -e
          echo
          sudo wget http://guest:guest@localhost:15672/cli/rabbitmqadmin -O /usr/local/bin/rabbitmqadmin
          sudo chmod +x /usr/local/bin/rabbitmqadmin
          # print logs from stdout (RABBITMQ_LOGS=-)
          docker logs --tail=20 rabbitmq
      - name: Print versions
        run: |
          # Print various binary versions
          git --version
          pip --version
          pip list
          which virtualenv
          virtualenv --version
          virtualenv/bin/virtualenv --version
          ls -l virtualenv/bin
          # Print out various environment variables info
          make play
      - name: make
        # use: script -e -c to print colors
        run: |
          ls -ld /tmp
          sudo ls -ld /tmp
          ls -l /tmp
          sudo ls -l /tmp
          mount
          sudo mount
          script -e -c "make ${TASK}"
      - name: Nightly
        # Run any additional nightly checks only as part of a nightly (cron) build
        if: "${{ env.IS_NIGHTLY_BUILD == 'yes' }}"
        run: |
          ./scripts/travis/run-nightly-make-task-if-exists.sh "${TASK}"
      - name: Codecov
        # NOTE: We only generate and submit coverage report for master and version branches and only when the build succeeds (default on GitHub Actions, this was not the case on Travis so we had to explicitly check success)
        if: "${{ success() && ((env.TASK == 'ci-unit') || (env.TASK == 'ci-integration')) && (env.ENABLE_COVERAGE == 'yes') }}"
        run: |
          ./scripts/travis/submit-codecov-coverage.sh
  slack-notification:
    name: Slack notification for failed master builds
    if: always()
    needs: ci
    runs-on: ubuntu-latest
    steps:
      - name: Workflow conclusion
        # this step creates an environment variable WORKFLOW_CONCLUSION and is the most reliable way to check the status of previous jobs
        uses: technote-space/workflow-conclusion-action@v2
      - name: CI Run Failure Slack Notification
        if: ${{ env.WORKFLOW_CONCLUSION == 'failure' && github.ref == 'refs/heads/master' }}
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
        uses: voxmedia/github-action-slack-notify-build@v1
        with:
          channel: development
          status: FAILED
          color: danger

      # HELPER FOR FUTURE DEVELOPERS:
      #  If your GitHub Actions job is failing and you need to debug it, by default there is
      #  no way to SSH into the container.
      #  The step below can be uncommeted and will stop here and allow you to SSH in.
      #  When this step is reached, simply refresh the GitHub Actions output for this build
      #  and this SSH command will be printed every 5 seconds to the output.
      #  Once you are done debugging in your SSH session, simply: touch /continue
      #  and this will continue the build.
      #
      # - name: Setup tmate session for debugging failed jobs (allows SSH into the container)
      #   uses: mxschmitt/action-tmate@v3
      #   if: "${{ failure() }}"
